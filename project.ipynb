{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ae01989",
   "metadata": {},
   "source": [
    "# DATA SCIENCE HACKATHON: INTELLIGENT SEARCH ENGINE\n",
    "This guide demonstrates solution for this competition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd33156",
   "metadata": {},
   "source": [
    "## Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd2e729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import multiprocessing\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "from evaluation import mean_average_precision\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from nltk.util import ngrams\n",
    "from spacy.lang.en import English\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "from utils.dataset import create_dataframe, split_sentence, split_words\n",
    "from utils.modelling import preparing_data_w2v\n",
    "from utils.prediction import search_similar_text\n",
    "from utils.processing import get_text, get_files_path\n",
    "from utils.vectors import get_avg_vector, postprocess_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da76caf5",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0a97ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "print('There are {} CPU cores.'. format(cores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7783cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_processes = cores - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8245c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy rule-based matching \n",
    "nlp_eng = English()\n",
    "sentencizer = nlp_eng.create_pipe(\"sentencizer\")\n",
    "nlp_eng.add_pipe(sentencizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4529fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_list = [\n",
    "    'Ashcroft', \n",
    "    'Density&Viscosity', \n",
    "    'Flow', \n",
    "    'Gas_analysis', \n",
    "    'Level', \n",
    "    'Liquid _analysis',\n",
    "    'Pressure', \n",
    "    'Temperature', \n",
    "    'Valves_actuators'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827966c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_data = './'\n",
    "folder_created_data = './'\n",
    "\n",
    "folder_json = 'text_json'\n",
    "folder_json_processed = 'data_text_processed'\n",
    "\n",
    "path_documents = './documents'\n",
    "\n",
    "dataset_texts = 'data_texts.pkl'\n",
    "\n",
    "dataset_names = 'device2document_map.pkl'\n",
    "\n",
    "dataset_vectors_w2v = 'data_vectors_w2v.pkl'\n",
    "dataset_vectors_ft = 'data_vectors_ft.pkl'\n",
    "\n",
    "model_name_w2v = 'word2vec.model'\n",
    "model_name_ft = 'fasttext.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5a476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_jsons = os.path.join(folder_data, folder_json)\n",
    "path_json_processed = os.path.join(folder_created_data, folder_json_processed)\n",
    "\n",
    "path_dataset_texts = os.path.join(folder_created_data, dataset_texts)\n",
    "\n",
    "path_dataset_names = os.path.join(folder_data, dataset_names)\n",
    "\n",
    "path_dataset_vectors_w2v = os.path.join(folder_created_data, dataset_vectors_w2v)\n",
    "path_dataset_vectors_ft = os.path.join(folder_created_data, dataset_vectors_ft)\n",
    "\n",
    "path_model_w2v = os.path.join(folder_created_data, model_name_w2v)\n",
    "path_model_ft = os.path.join(folder_created_data, model_name_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b212e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for multiprocessing\n",
    "def _apply_df(args):\n",
    "    df, func, kwargs = args\n",
    "    return df.apply(func, **kwargs)\n",
    "\n",
    "\n",
    "def apply_by_multiprocessing(df, func, **kwargs):\n",
    "    workers = kwargs.pop('workers')\n",
    "    pool = multiprocessing.Pool(processes=workers)\n",
    "    result = pool.map(_apply_df, [(d, func, kwargs)\n",
    "            for d in np.array_split(df, workers)])\n",
    "    pool.close()\n",
    "    return pd.concat(list(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daa8d4d",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "After passing flow  PDF documents are converted into following data sets:\n",
    "<li>documents (pdf);</li>\n",
    "<li>document pages (images);</li>\n",
    "<li>document texts with coordinates (json files);</li>\n",
    "<li>texts embeddings (pickle file);</li>\n",
    "<li>company and device names (pickle file).</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f6a57d",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c1c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 cp s3://dsg-hackathon-dataset/text_json.zip .\n",
    "# !unzip text_json.zip\n",
    "# !aws s3 cp s3://dsg-hackathon-dataset/documents.zip .\n",
    "# !unzip documents.zip -d documents\n",
    "# !aws s3 cp s3://dsg-hackathon-dataset/validation.csv ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de7d2aa",
   "metadata": {},
   "source": [
    "#### File Usage\n",
    "\n",
    "There are 3 files provided for the baseline solution:\n",
    "<li>text_json - folder contains parsed text. The text was parsed by Cloud Vision API. Apart from texts the model also returns text coordinates on page;</li>\n",
    "<li>documents - folder contains PDF documents;</li>\n",
    "<li>validation.csv - validation queries for local validation</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46777ce",
   "metadata": {},
   "source": [
    "### Processing all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf4968d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking how many files in respective folders\n",
    "for fold, f_list in get_files_path(path_jsons, folder_list).items():\n",
    "    print('Folder: {}.\\nNumber files for processing: {} \\n'.format(fold, len(f_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9097874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create processed json files by respective folders\n",
    "files_list_dict = get_files_path(path_jsons, folder_list)\n",
    "partial_args = partial(get_text, folder_save=path_json_processed, lang_threshold=0.5)\n",
    "with Pool(processes=number_processes) as pool:\n",
    "    %time pool.map(partial_args, list(files_list_dict.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173c8c72",
   "metadata": {},
   "source": [
    "### Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c661fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_list = []\n",
    "folder_exist_list = [item.split('.')[0] for item in os.listdir(path_json_processed)]\n",
    "\n",
    "for folder in tqdm(folder_exist_list):\n",
    "    if folder in folder_list:\n",
    "        file_processed_path = '{}/{}.json'.format(path_json_processed, folder)\n",
    "        dataframe_list.append(create_dataframe(file_processed_path, \n",
    "                                               doc_path_folder='s3://hackathon-baseline/duai_docs/'))\n",
    "data = pd.concat(dataframe_list).reset_index(drop=True)\n",
    "print('There are {} rows in created DataFrame'.format(data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e65b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text into sentences\n",
    "%time data['text_sentences'] = apply_by_multiprocessing(data['text'], partial(split_sentence, nlp=nlp_eng, \n",
    "                                                                    sentence_length=10), workers=number_processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf221b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split sentences into words\n",
    "%time data['text_words'] = data['text_sentences'].apply(lambda t: [split_words(sent, nlp_eng, 2) for sent in t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ac967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove records without sentences\n",
    "data = data[data['text_words'].apply(len)!=0].reset_index(drop=True)\n",
    "print('There are {} rows after removing records without sentences...'.format(data.shape[0]))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed3c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataset to pickle file\n",
    "data.to_pickle(path_dataset_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1def12c8",
   "metadata": {},
   "source": [
    "## Vector Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09424e5e",
   "metadata": {},
   "source": [
    "#### Dataset initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f957433",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(path_dataset_texts)\n",
    "print('There are {} rows in loaded DataFrame'.format(data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7ac1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ed8561",
   "metadata": {},
   "source": [
    "#### Text preprocessing\n",
    "Text preprocessing steps for Word2Vec models include:\n",
    "<li>removing stop words;</li>\n",
    "<li>removing numbers and words with numbers;</li>\n",
    "<li>removing short words.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5778672",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time sentences_list = preparing_data_w2v(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b643716",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete very short words\n",
    "sentences_list = [[word for word in sent if len(word)  > 2 ]\n",
    "                                for sent in sentences_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ec34e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of sentences for modelling \n",
    "print(sentences_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9270e8",
   "metadata": {},
   "source": [
    "### Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9797cc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v = Word2Vec(min_count=4,\n",
    "                 window=3,\n",
    "                 size=300,\n",
    "                 sample=6e-5, \n",
    "                 alpha=0.025, \n",
    "                 min_alpha=0.0001, \n",
    "                 negative=20,\n",
    "                 workers=cores-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b11572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary for Word2Vec\n",
    "%time model_w2v.build_vocab(sentences_list, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266e2961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Word2Vec model\n",
    "%time model_w2v.train(sentences_list, total_examples=model_w2v.corpus_count, epochs=60, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996fac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save Word2Vec model\n",
    "model_w2v.save(path_model_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e97df5",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bb5527",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = FastText(min_count=3,\n",
    "                 window=4,\n",
    "                 size=300,\n",
    "                 sample=6e-5,\n",
    "                 alpha=0.025,\n",
    "                 min_alpha=0.0001,\n",
    "                 negative=20,\n",
    "                 workers=cores-2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a378be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary for FastText\n",
    "%time model_ft.build_vocab(sentences_list, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a297345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train FastText model\n",
    "%time model_ft.train(sentences_list, total_examples=model_ft.corpus_count, epochs=100, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c7253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save FastText model\n",
    "model_ft.save(path_model_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e168db88",
   "metadata": {},
   "source": [
    "## Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b1491a",
   "metadata": {},
   "source": [
    "#### Dataset initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a342bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(path_dataset_texts)\n",
    "print('There are {} rows in loaded DataFrame'.format(data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8518e8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e343f6",
   "metadata": {},
   "source": [
    "#### Models initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65236fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v = Word2Vec.load(path_model_w2v)\n",
    "print('Word2Vec Model Initialization: {}'.format(model_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbcccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = FastText.load(path_model_ft)\n",
    "print('FastText Model Initialization: {}'.format(model_ft))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d6609b",
   "metadata": {},
   "source": [
    "Explanation of variables:\n",
    "<li>data - pd.DataFrame where save vectors which build by Word2Vec model;</li>\n",
    "<li>data_2 - pd.DataFrame where save vectors which build by FastText model.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6bff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep copy pd.DataFrame \n",
    "data_2 = data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90cd96f",
   "metadata": {},
   "source": [
    "#### Creating vectors for Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9027cdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time data['text_vectors'] = data['text_words'].apply(lambda x: [get_avg_vector(sent, model_w2v) for sent in x])\n",
    "%time data['sentences'] = data.apply(lambda x: list(zip(x['text_sentences'], x['text_vectors'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5ff916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if number of sentences equal to number of vectors\n",
    "data[data['text_sentences'].apply(len)!=data['text_vectors'].apply(len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095adeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time data_vectors = postprocess_vectors(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dd6fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save pd.DataFrame with text vectors to pickle file\n",
    "data_vectors.to_pickle(path_dataset_vectors_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648ae782",
   "metadata": {},
   "source": [
    "#### Creating vectors for Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064d3aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time data_2['text_vectors'] = data['text_words'].apply(lambda x: [get_avg_vector(sent, model_ft) for sent in x])\n",
    "%time data_2['sentences'] = data.apply(lambda x: list(zip(x['text_sentences'], x['text_vectors'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c652f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if number of sentences equal to number of vectors\n",
    "data_2[data_2['text_sentences'].apply(len)!=data_2['text_vectors'].apply(len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f467bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time data_vectors_2 = postprocess_vectors(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ac27c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save pd.DataFrame with text vectors to pickle file\n",
    "data_vectors_2.to_pickle(path_dataset_vectors_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a280a22a",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b08f99",
   "metadata": {},
   "source": [
    "### Create document to device mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b52ae41",
   "metadata": {},
   "source": [
    "Each document from the dataset contains information about single or multiple  devices and manufacturer companies. In most cases this information is located on the first pages. But very often device models appear in text a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd2eed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_BLACK_LIST = ['', 'installation', 'manual', 'manuals', 'en', 'transmitters', 'ultrasonic',\n",
    "                   'quick', 'start', 'foundation', 'model', 'guide', 'rev', 'series', 'gas', 'sensor',\n",
    "                   'transmitter', 'meters', 'pressure', 'supplement', 'shafer', 'replacement', 'protocol',\n",
    "                   'instructions', 'instructions', 'service', 'control', 'configuration', 'operation',\n",
    "                   'power', 'procedure', 'instruction', 'maintenance', 'level', 'guides', 'meter', 'analyzer']\n",
    "\n",
    "file_dict = {folder: os.listdir(os.path.join(path_jsons, folder)) for folder in os.listdir(path_jsons)[1:]}\n",
    "\n",
    "            \n",
    "def filter_word_freq(freq_dict, threshold):\n",
    "    return {item: freq for item, freq in freq_dict.items() if freq > threshold} \n",
    "\n",
    "def sort_dict(freq_dict):\n",
    "    return {k: v for k, v in sorted(freq_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "device_model_pattern = r'\\d{3,4}\\w{0,2}'\n",
    "words = '-'.join(list(chain(*file_dict.values())))\n",
    "\n",
    "device_models_freq = Counter(ngrams(re.findall(device_model_pattern, words), 1))\n",
    "device_models_freq = sort_dict(filter_word_freq(device_models_freq, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b352a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_freq = Counter(ngrams([w for w in re.sub(device_model_pattern, '', words).replace('.pdf', '').split('-') if w not in WORD_BLACK_LIST and len(w) > 4], 1))\n",
    "unigram_freq = filter_word_freq(unigram_freq, 8)\n",
    "bigram_freq = Counter(ngrams([w for w in re.sub(device_model_pattern, '', words).replace('.pdf', '').split('-') if w not in WORD_BLACK_LIST and len(w) > 4], 2))\n",
    "bigram_freq = filter_word_freq(bigram_freq, 4)\n",
    "\n",
    "unigram2del = []\n",
    "for manufacturer, freq in bigram_freq.items():\n",
    "    w1 = (manufacturer[0],)\n",
    "    w2 = (manufacturer[1],)\n",
    "    if w1 in unigram_freq.keys() and w2 in unigram_freq.keys():\n",
    "        if freq >= unigram_freq[w1] or freq >= unigram_freq[w2]:\n",
    "            unigram2del.extend([w1, w2])\n",
    "clean_unigram_freq = {manufacturer: freq for manufacturer, freq in \n",
    "                      unigram_freq.items() if manufacturer not in unigram2del}\n",
    "\n",
    "manufacturer_freq = sort_dict(dict(chain.from_iterable(d.items() for d in (unigram_freq, bigram_freq))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c980597",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_map = []\n",
    "for folder, docs in file_dict.items():\n",
    "    for doc in docs:\n",
    "        clean_doc = doc.replace('-', ' ')\n",
    "        \n",
    "        device_model = []\n",
    "        prev_model_freq = 0\n",
    "        for prob_device_model, freq in device_models_freq.items():\n",
    "            if (prob_device_model[0] in clean_doc) and (freq >= prev_model_freq):\n",
    "                device_model.append(prob_device_model[0])\n",
    "                prev_model_freq = freq\n",
    "            elif freq < prev_model_freq:\n",
    "                break\n",
    "        \n",
    "        manufacturer = ''\n",
    "        for prob_manufacturer in manufacturer_freq.keys():\n",
    "            prob_manufacturer = ' '.join(prob_manufacturer)\n",
    "            if prob_manufacturer in clean_doc:\n",
    "                manufacturer  = prob_manufacturer\n",
    "                break\n",
    "                \n",
    "        doc_map.append({'doc_name': doc, 'doc_class': folder, 'manufacturer': manufacturer, 'device_model': device_model})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c04ebca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_names = pd.DataFrame(doc_map)\n",
    "data_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6814cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save pd.DataFrame with device information to pickle file\n",
    "data_names.to_pickle(path_dataset_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ed706b",
   "metadata": {},
   "source": [
    "### Data & Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38160e87",
   "metadata": {},
   "source": [
    "#### vectors dataset initialization for Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c589bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vectors = pd.read_pickle(path_dataset_vectors_w2v)\n",
    "print('The dimensionality of the DataFrame for Word2Vec model: {}'.format(data_vectors.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b621fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vectors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3a0356",
   "metadata": {},
   "source": [
    "#### vectors dataset initialization for FastText model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dec6a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vectors_2 = pd.read_pickle(path_dataset_vectors_ft)\n",
    "print('The dimensionality of the DataFrame for ft: {}'.format(data_vectors_2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76c40e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vectors_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61c4ee3",
   "metadata": {},
   "source": [
    "#### Models initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ab018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v = Word2Vec.load(path_model_w2v)\n",
    "print('Word2Vec Model Initialization: {}'.format(model_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ef98e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = FastText.load(path_model_ft)\n",
    "print('FastText Model Initialization: {}'.format(model_ft))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460df8de",
   "metadata": {},
   "source": [
    "#### Names dataset initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c62d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_names = pd.read_pickle(path_dataset_names)\n",
    "print('The dimensionality of the DataFrame: {}'.format(data_names.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ce0569",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_names.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e819b6a2",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b84c984",
   "metadata": {},
   "source": [
    "Let’s try to find relevant places in different documents for a user's queries. To retrieve best similar texts for a new query we would have to:\n",
    "<li>Split the query into: text, company and device name.</li>\n",
    "<li>Encode query text into the same model we used for vectors creating.</li>\n",
    "<li>Filter texts based on company and device names.</li>\n",
    "<li>Retrieve most similar text chunks and IDs.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400a6ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_query(x):\n",
    "    manufacturer = ''\n",
    "    device_model = ''\n",
    "    text = ''\n",
    "    digit_pattern = r'\\s\\d{3,}\\w{0,2}'\n",
    "    if re.findall(digit_pattern, x):\n",
    "        groups = re.split(digit_pattern, x)\n",
    "        manufacturer = groups[0].strip()\n",
    "        text = groups[1].strip()\n",
    "        device_model = re.findall(digit_pattern, x)[0].strip()\n",
    "    else:\n",
    "        special_cases = ['burner', 'flow meter', 'hydrocarbon analyzer', 'liquid analyzer',\n",
    "                         'pressure meter', 'temperature meter', 'viscosity meter', 'eho bettis',\n",
    "                         'eho', 'level meter']\n",
    "        for case in special_cases:\n",
    "            result = re.match(case, x)\n",
    "            if result is not None:\n",
    "                manufacturer = case\n",
    "                text = x[result.end()+1:].strip()\n",
    "                break\n",
    "            \n",
    "    return pd.Series([manufacturer, device_model, text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddfdfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation dataset initialization\n",
    "validation_df = pd.read_csv('validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1a90d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b958b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test queries dataset\n",
    "data_input = pd.Series(validation_df['query'].unique()).apply(lambda x: split_query(x))\n",
    "data_input.rename(columns={0: 'manufacturer', 1: 'device_model', 2: 'text'}, inplace=True)\n",
    "data_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f68126f",
   "metadata": {},
   "source": [
    "How does predictions dataset build?\n",
    "   1. Find top_n similar text for Word2Vec and FastText models. \n",
    "   2. Concatenation prediction for 2 models and sort by similarity.\n",
    "   3. Get only top_n first values in this created dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798eb4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(data_input, data_vectors, data_vectors_2, data_names, model_w2v, model_ft, nlp, top_n = 5):\n",
    "    outputs = []\n",
    "    for i in range(len(data_input)):\n",
    "        query_input = data_input.iloc[i]\n",
    "        query_text = query_input['manufacturer'] + ' ' + query_input['device_model'] + ' ' + query_input['text']\n",
    "        data_output_w2v = search_similar_text(query_input, data_vectors, data_names,\n",
    "                                              'text_vectors', model_w2v, nlp, top_n)\n",
    "        data_output_ft = search_similar_text(query_input, data_vectors_2, data_names,\n",
    "                                              'text_vectors', model_ft, nlp, top_n)\n",
    "        data_output = pd.concat([data_output_w2v,data_output_ft]).sort_values(by=['similarity'],ascending=False)[:top_n]\n",
    "        data_output['query'] = [query_text for _ in range(1, top_n+1)]\n",
    "        data_output['top_n'] = [i for i in range(1, top_n+1)]\n",
    "        outputs.append(data_output)\n",
    "    \n",
    "    return pd.concat(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6c3eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for test queries\n",
    "predictions = get_prediction(data_input, data_vectors, data_vectors_2, data_names,\n",
    "                                   model_w2v, model_ft, nlp_eng, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466ecdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d23f9a",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Your model will to be evaluated with Mean Average Precision at 5 (MAP@5) metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d098aebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(predictions: pd.DataFrame):\n",
    "    predictions[['doc_path', 'text_page']] = predictions['page_class_coordinate'].apply(lambda x: pd.Series(x[0][0].split('.pdf_')))\n",
    "    predictions['doc_path'] =  predictions['doc_path'] + '.pdf'\n",
    "    return predictions[['query', 'top_n', 'doc_path', 'text_page']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cbf43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create submission pd.DataFrame from predictions \n",
    "submission_df = create_submission(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2a7de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c1d6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Average Precision metric for this model\n",
    "mean_average_precision(validation_df, submission_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_kernel_name",
   "language": "python",
   "name": "pytorch_kernel_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
